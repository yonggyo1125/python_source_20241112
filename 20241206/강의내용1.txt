데이터 전처리
0. 훈련 세트, 테스트 세트 

1. 키워드 정리
1) 데이터 전처리
- 머신러닝 모델 훈련 데이터를 주입하기 전에 가공하는 단계를 말합니다.
- 때로는 데이터 전처리에 많은 시간이 소모되고 합니다.
2) 표준 점수
- 훈련 세트의 스케일을 바꾸는 대표적인 방법 중 하나입니다.
- 표준점수는 특성의 평균을 빼고 표준편차로 나눕니다.
- 반드시 훈련 세트의 평균과 표준편차로 테스트 세트를 바꿔야 합니다.
3) 브로드 캐스팅
- 크기가 다른 넘파이 배열에서 자동으로 사칙 연산을 모든 행이나 열로 확장하여 수행하는 기능입니다.

참고)
	리스트, 튜플만 가능 
		zip( 리스트 또는 튜플)
	
	넘파이 
		column_stack(튜플(..,...))
		ones(숫자) :  숫자만큼 1로 채운 1차원 넘파이 배열
		zeros(숫자) : 숫자만큼 0으로 채운 1차원 넘파이 배열
		concatenate(넘파이 배열, ....) : 하나의 넘파이 배열로 합치기
		mean() : 평균, axis - 0 : 행기준에서 평균, 1 - 열 기준에서 평균
		std() : 표준 편차, axis - 0: 행기준에서 표준편차, 1 - 열 기준에서 평균
		
2. 넘파이로 데이터 준비하기
3. 사이킷런으로 훈련 세트와 테스트 세트 나누기
4. 수상한 도미 한마리
5. 기준을 맞춰라
	- 표준 점수
	- (원 점수 - 평균) / 표준 편차
	- 데이터의 기준을 통일하기 위한 목적 
	
	- 표준 편차 : 데이터가 평균에서 얼만큼 떨어져 있는지를 표현한 값 
					 표준 편차가 크다 - 데이터가 평균서 들쑥 날쑥 분포
					 표준 편차가 작다 - 데이터가 평균에서 고르게 분포
					 
					 
					 
					분산 
						(원 점수 - 평균)^2 모두 더한 값 
						-----------------------------------
						총 갯수
					표준 편차 
						분산을 제곱근한 표준편차
					
					표준 점수
						(원 점수 - 평균) / 표준 편차
						
6. 전처리 데이터로 모델 훈련하기
numpy 
	mean(..) : 넘파이 배열의 평균, 다차원 배열일 때 편리하게 차원별로 평균을 구하기 쉽다.
	std() : 표준 편차




지도 학습 
	- 정답이 있는 학습 
	- 학습 데이터, 정답 데이터(타깃)
	- K-최근접 이웃 분류(KNeighborsClassifier)
	
	1) 학습 데이터와 테스트 데이터는 분리
		- 테스트 데이터는 원 데이터의 20%
		- 학습 데이터와 테스트 데이터는 고르게 섞여 있어야 편향된 학습 X(편향된 학습 - 샘플링 편향)
		- 단위가 다른 특성을 가지고 예측을하면 큰 수를 가진 특성에 예측이 편향 될 수 있다!
		- 특성을 전부 동일한 기준으로 변경 -  표준 점수!
	
비지도학습 
	- 정답이 없는 학습 
	- 군집 알고리즘, k-평균